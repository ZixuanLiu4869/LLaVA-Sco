LLaVA-SCo: Teach Vision Language Model to Self-Correct



Dataset

You can download the dataset from: https://drive.google.com/file/d/1937sO0WJwKIAT1661wk7jCgw4-b_kAyO/view?usp=sharing


Finetuning

We are uing llama-recipes: https://github.com/meta-llama/llama-cookbook


git clone 



First, download the image of LLaVA-CoT from https://huggingface.co/datasets/Xkev/LLaVA-CoT-100k

Next, change train
